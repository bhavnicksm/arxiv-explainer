{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://arxiv.org/pdf/2302.01318\")\n",
    "pdf_content = r.content\n",
    "pdf_file = io.BytesIO(pdf_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-2-3\n",
      "Accelerating Large Language Model Decoding\n",
      "with Speculative Sampling\n",
      "1 1 1 1 1\n",
      "CharlieChen ,SebastianBorgeaud ,GeoffreyIrving ,Jean-BaptisteLespiau ,LaurentSifre andJohn\n",
      "1\n",
      "Jumper\n",
      "1AllauthorsfromDeepMind\n",
      "We present speculative sampling, an algorithm for accelerating transformer decoding by enabling the\n",
      "generationofmultipletokensfromeachtransformercall. Ouralgorithmreliesontheobservationthat\n",
      "the latency of parallel scoring of short continuations, generated by a faster but less powerful draft\n",
      "model,iscomparabletothatofsamplingasingletokenfromthelargertargetmodel. Thisiscombined\n",
      "with a novel modified rejection sampling scheme which preserves the distribution of the target model 3202\n",
      "withinhardwarenumerics. WebenchmarkspeculativesamplingwithChinchilla,a70billionparameter\n",
      "language model, achieving a 2‚Äì2.5√ó decoding speedup in a distributed setup, without compromising\n",
      "thesamplequalityormakingmodificationstothemodelitself.\n",
      "beF\n",
      "Introduction 2\n",
      "Scaling transformer models to 500B+ parameters has led to large performance improvements on ]LC.sc[\n",
      "manynaturallanguage,computervisionandreinforcementlearningtasks(Arnabetal.,2021;Brown\n",
      "etal.,2020;Chowdheryetal.,2022;Dosovitskiyetal.,2020;Hoffmannetal.,2022;Raeetal.,2021).\n",
      "However, transformer decoding remains a highly costly and inefficient process in this regime.\n",
      "Transformer sampling is typically memory bandwidth bound (Shazeer, 2019), so for a given set of 1v81310.2032:viXra\n",
      "hardware, the time to generate a single token in transformer models is proportional to a first order\n",
      "approximationtothesizeofparametersandthesizeofthetransformermemory. Thesizeoflanguage\n",
      "models also necessitates serving with model parallelism ‚Äì adding communication overheads (Pope\n",
      "et al., 2022) and multiplying resource requirements. Since each new token depends on the past,\n",
      "many such transformer calls are required to sample a new sequence.\n",
      "We present an algorithm to accelerate transformer sampling for latency critical applications, which\n",
      "we call speculative sampling (SpS). This is achieved by:\n",
      "1. Generating a short draft of length ùêæ. This can be attained with either a parallel model (Stern\n",
      "et al., 2018) or by calling a faster, auto-regressive model ùêæ times. We shall refer to this model\n",
      "as the draft model, and focus on the case where it is auto-regressive.\n",
      "2. Scoringthedraftusingthelarger,morepowerfulmodelfromwewishtosamplefrom. Weshall\n",
      "refer to this model as the target model.\n",
      "3. Using a modified rejection sampling scheme, accept a subset of the ùêæ draft tokens from left to\n",
      "right, recovering the distribution of the target model in the process.\n",
      "Intuitively, there are often sequences where the next token might be ‚Äúobvious‚Äù. Therefore, if there is\n",
      "strongagreementbetweenthedraftandtargetmodel‚Äôsdistributionsonagiventokenorsub-sequence\n",
      "of tokens, this setup permits the generation of multiple tokens each time the target model is called.\n",
      "We show that the expected acceptance rate of draft tokens is sufficient to offset the overhead of the\n",
      "Correspondingauthor(s):ccharlie@deepmind.com\n",
      "¬© 2023DeepMind.AllrightsreservedAcceleratingLargeLanguageModelDecodingwithSpeculativeSampling\n",
      "draftingprocessforlargelanguagemodels,resultinginaneffectiveandpracticalmethodforreducing\n",
      "sampling latency without the need for modifying the target model or biasing the sample distribution.\n",
      "Depending on the evaluation domain, SpS leads to a 2‚Äì2.5√ó speedup when sampling from Chinchilla\n",
      "(Hoffmann et al., 2022). Notably, the mean tokens per second with SpS often exceeds the idealised\n",
      "ceiling on auto-regressive sampling speed imposed by the memory bandwidth.\n",
      "Related Work\n",
      "There has been a substantial body of work focused on improving sampling latency of large transform-\n",
      "ers and other auto-regressive models.\n",
      "Since sampling performance is heavily coupled with the model size in memory, quantisation to\n",
      "int8 or even int4 (Dettmers et al., 2022; Yao et al., 2022) and distillation (Jiao et al., 2020; Sanh\n",
      "et al., 2019) of transformers are effective techniques for reducing sampling latency with little to no\n",
      "performance penalty. The observation that model size contributes less to the final performance than\n",
      "expected (Hoffmann et al., 2022) has also encouraged smaller language models in general.\n",
      "During sampling, a cache of the keys and values is maintained for every attention layer, and could\n",
      "become a memory bandwidth bottleneck as the batch size increases. Methods such as multi-query\n",
      "attention (Shazeer, 2019) aims to improve sampling performance by shrinking this cache. However\n",
      "thesetechniquesaremosteffectiveatmaximisingthroughout(atlargerbatchsizes)insteadoflatency,\n",
      "especiallyforlargermodelswherethemajorityofthememorybandwidthbudgetisconsumedbythe\n",
      "parameters.\n",
      "Using a combination of the above techniques, in addition to a number of low-level optimisations to\n",
      "TPUs, Pope et al. (2022) have greatly improved the serving latency and efficiency of PaLM 540B.\n",
      "There is an existing body of similar work exploiting the efficiency of transformers and sequence\n",
      "models operating in parallel. This includes block parallel sampling (Stern et al., 2018), aggressive\n",
      "decoding (Ge et al., 2022), in addition to some work in parallelizing autoregressive models in the\n",
      "image domain (Song et al., 2021; Wiggers and Hoogeboom, 2020). These methods have yet to be\n",
      "adapted to typical language model use-cases since they either only work with greedy sampling, bias\n",
      "the results or are focused on other modalities. Further, to our knowledge none of these techniques\n",
      "have been scaled to distributed setups, which is necessary for the most expensive decoders with the\n",
      "tens or hundreds of billions of parameters.\n",
      "Coincidentally, the work in this manuscript was undertaken concurrently and independently of\n",
      "theworkonspeculativedecodingfromLeviathanetal.(2022). Wefocusmoreheavilythedistributed\n",
      "serving setting for large models and offer some incremental optimisations, but otherwise the core\n",
      "underlying idea is the same.\n",
      "Auto-regressive Sampling\n",
      "WhilsttransformerscanbetrainedefficientlyandinparallelonTPUsandGPUs,samplesaretypically\n",
      "drawn auto-regressively (See algorithm 1). For most applications, auto-regressive sampling (ArS) is\n",
      "highlymemorybandwidthboundandthuscannotmakeeffectiveuseofmodernacceleratorhardware\n",
      "(Shazeer, 2019). A memory bound model call only generates a single token for every sequence in the\n",
      "batch, hence generating multiple tokens introduces a large amount of latency in any system which\n",
      "makes use of it.\n",
      "2AcceleratingLargeLanguageModelDecodingwithSpeculativeSampling\n",
      "Thisisespeciallyproblematicasthenumberofparametersinthemodelincreases. Sinceallthemodel\n",
      "parameters need to pass through at least one accelerator chip, the model size divided by the total\n",
      "memory bandwidth across all chips gives us a hard ceiling on the maximum auto-regressive sampling\n",
      "speed. Larger models also require serving on multiple accelerators, introducing a further source of\n",
      "latency due to inter-device communication overheads.\n",
      "Algorithm 1 Auto-regressive (ArS) with Auto-Regressive Models\n",
      "Givenauto-regressivetargetmodelùëû(.|.) andinitialpromptsequence ùë• 1,...,ùë• andtargetsequence\n",
      "ùë°\n",
      "lengthùëá.\n",
      "Initialise ùëõ ‚Üê ùë°.\n",
      "while ùëõ <ùëá do\n",
      "Sample ùë• ùëõ+1 ‚àº ùëû(ùë•|ùë• 1,...,ùë• ùëõ)\n",
      "ùëõ ‚Üê ùëõ+1\n",
      "end while\n",
      "Algorithm 2 Speculative Sampling (SpS) with Auto-Regressive Target and Draft Models\n",
      "Given lookahead ùêæ and minimum target sequence lengthùëá.\n",
      "Given auto-regressive target model ùëû(.|.), and auto-regressive draft model ùëù(.|.), initial prompt\n",
      "sequence ùë• ,...,ùë• .\n",
      "0 ùë°\n",
      "Initialise ùëõ ‚Üê ùë°.\n",
      "while ùëõ <ùëá do\n",
      "for ùë° = 1 : ùêæ do\n",
      "Sample draft auto-regressively ùë•Àú ‚àº ùëù(ùë•|,ùë• 1,...,ùë• ùëõ,ùë•Àú 1,...,ùë•Àú ùë°‚àí1)\n",
      "ùë°\n",
      "end for\n",
      "In parallel, compute ùêæ+1 sets of logits from drafts ùë•Àú 1,...,ùë•Àú :\n",
      "ùêæ\n",
      "ùëû(ùë•|,ùë• 1,...,ùë• ùëõ), ùëû(ùë•|,ùë• 1,...,ùë• ùëõ,ùë•Àú 1), ..., ùëû(ùë•|,ùë• 1,...,ùë• ùëõ,ùë•Àú 1,...,ùë•Àú ùêæ)\n",
      "for ùë° = 1 : ùêæ do\n",
      "Sample ùëü ‚àºùëà[0,1] from a uniform distribution.\n",
      "(cid:16) (cid:17)\n",
      "if ùëü < min 1, ùëû(ùë•|ùë•1,...,ùë• ùëõ+ùë°‚àí1) , then\n",
      "ùëù(ùë•|ùë•1,...,ùë• ùëõ+ùë°‚àí1)\n",
      "Set ùë• ‚Üê ùë•Àú and ùëõ ‚Üê ùëõ+1.\n",
      "ùëõ+ùë° ùë°\n",
      "else\n",
      "sample ùë• ‚àº (ùëû(ùë•|ùë• 1,...,ùë• ùëõ+ùë°‚àí1)‚àí ùëù(ùë•|ùë• 1,...,ùë• ùëõ+ùë°‚àí1)) and exit for loop.\n",
      "ùëõ+ùë° +\n",
      "end if\n",
      "end for\n",
      "If all tokens ùë• ùëõ+1,...,ùë• ùëõ+ùêæ are accepted, sample extra token ùë• ùëõ+ùêæ+1 ‚àº ùëû(ùë•|,ùë• 1,...,ùë• ùëõ,ùë• ùëõ+ùêæ) and\n",
      "set ùëõ ‚Üê ùëõ+1.\n",
      "end while\n",
      "Speculative Sampling\n",
      "Conditional Scoring\n",
      "For speculative sampling (See algorithm 2), we first make the observation that computing the logits\n",
      "of a short continuation of ùêæ tokens in parallel has a very similar latency to that of sampling a single\n",
      "3AcceleratingLargeLanguageModelDecodingwithSpeculativeSampling\n",
      "token. We focus our attention on large transformers, sharded in the Megatron style (Shoeybi et al.,\n",
      "2019). For these models the majority of sampling time can be attributed to three components:\n",
      "1. Linear Layers: For small batch sizes, each linear layer only processes a small number of\n",
      "embeddings. This causes the dense matrix multiplies in the feed-forward layers, queries, keys,\n",
      "values computations and the final attention projection to become memory bound. For small ùêæ,\n",
      "this will continue to be memory bound and therefore take a similar amount of time.\n",
      "2. TheAttentionMechanism: Theattentionmechanismisalsomemorybound. Duringsampling,\n",
      "we maintain a cache of all the keys and values of the previous tokens in the sequence to avoid\n",
      "re-computation. These KV-caches are large, and accounts for the majority of the memory\n",
      "bandwidth utilisation for the attention mechanism. However, since the KV-cache size does not\n",
      "change as we increase ùêæ, there is little to no delta in this component.\n",
      "3. All-reduces: Asmodelsgrowinsize,itsparametersneedtobedividedacrossmultipleaccelera-\n",
      "tors, leading to communication overheads. With Megatron, this manifests itself as an all-reduce\n",
      "after every feed-forward and attention layer. Since only the activations for a small number of\n",
      "tokens are transmitted, this operation is typically latency bound instead of throughput bound\n",
      "for both sampling and scoring (for small ùêæ). Again, this results in a similar amount of time\n",
      "spent in the two cases.\n",
      "Other sources of overhead may exist, depending on the exact transformer implementation. Therefore\n",
      "it is still possible that the choice of positioning encoding, decoding method (e.g. a sort might be\n",
      "required for nucleus sampling), hardware limitations etc. can introduce some deltas between scoring\n",
      "and sampling. However, if the conditions are met such that the above components dominate then\n",
      "scoring should not be significantly slower for small ùêæ.\n",
      "Modified Rejection Sampling\n",
      "We require a method to recover the distribution of the target model from samples from the draft\n",
      "model, and logits of said tokens from both models.\n",
      "To achieve this, we introduce the following rejection sampling scheme of the drafted tokens. Given a\n",
      "sequence of tokens ùë• 1,...,ùë• ùëõ, and ùêæ draft tokens ùë•Àú ùëõ+1,...,ùë•Àú generated from ùëù(.|.), we accept ùë•Àú\n",
      "ùëõ+ùêæ ùëõ+1\n",
      "with probability:\n",
      "min(cid:18) ùëû(ùë•Àú ùëõ+1|ùë• 1,...,ùë• ùëõ)(cid:19)\n",
      "1,\n",
      "ùëù(ùë•Àú ùëõ+1|ùë• 1,...,ùë• ùëõ)\n",
      "Whereùëû(ùë•Àú ùëõ+1|ùë• 1,...,ùë• ùëõ) and ùëù(ùë•Àú ùëõ+1|ùë• 1,...,ùë• ùëõ) aretheprobabilityof ùë•Àú ùëõ+1 accordingtothetargetand\n",
      "draft models respectively, conditioned on the context so far.\n",
      "If the token is accepted, we set ùë• ùëõ+1 ‚Üê ùë•Àú ùëõ+1 and repeat the process for ùë•Àú ùëõ+2 until either a token\n",
      "is rejected or all tokens have been accepted.\n",
      "If ùë•Àú is rejected, we resample ùë• from the following distribution:\n",
      "ùëõ+1 ùëõ+1\n",
      "ùë• ùëõ+1 ‚àº (ùëû(ùë•|ùë• 1,...,ùë• ùëõ)‚àí ùëù(ùë•|ùë• 1,...,ùë• ùëõ)) +\n",
      "Where (.) denotes:\n",
      "+\n",
      "max(0, ùëì(ùë•))\n",
      "(ùëì(ùë•)) =\n",
      "+ (cid:205) max(0, ùëì(ùë•))\n",
      "ùë•\n",
      "By applying this sequentially, we recover the distribution of the target model for the accepted tokens\n",
      "(see proof in Theorem 1) within hardware numerics. Note that:\n",
      "4AcceleratingLargeLanguageModelDecodingwithSpeculativeSampling\n",
      "‚Ä¢ At least one token will always be generated from a draft-accept loop ‚Äì if the first token is\n",
      "rejected, a valid token is resampled.\n",
      "‚Ä¢ Since the final token of the draft gives us the logits for the next token, if every drafted token is\n",
      "accepted, we can sample from it normally. This gives us a maximum of ùêæ+1 tokens per loop,\n",
      "over the naive implementation which would only return ùêæ tokens.\n",
      "With standard sampling methods such as nucleus, top-k sampling and adjusting temperature, we\n",
      "can modify the probabilities accordingly before applying this rejection sampling scheme. We have\n",
      "observed that the overall acceptance rate is robust to the exact parameters used.\n",
      "Because we do not interact with the body of the transformer itself, this method can be used in\n",
      "conjunction many other techniques for accelerating or optimising the memory use of sampling, such\n",
      "as quantisation and multi-query attention.\n",
      "Choice of Draft Models\n",
      "Since the acceptance criterion guarantees the distribution of the target model in our samples, we are\n",
      "free to choose the method for drafting a continuation as long as it exposes logits, and there is a high\n",
      "enough acceptance rate and/or low enough latency to break-even. There exist several approaches\n",
      "here:\n",
      "‚Ä¢ Incorporating draft generation into the target model, and train the model from the start. This\n",
      "is the strategy used by Stern et al. (2018), which adds multiple heads into the transformer to\n",
      "generate multiple tokens.\n",
      "‚Ä¢ Using sequence level distillation (Kim and Rush, 2016) to generate a second model which\n",
      "predicts ùêæ tokens in parallel. This strategy was employed by Ge et al. (2022).\n",
      "‚Ä¢ Set a portion of the activations of the target modelas an inputto the draft model, andtrain the\n",
      "draft model with this input.\n",
      "Although these methods will likely yield powerful drafts, they require a large number of data gener-\n",
      "ated from the target model or changes to the target model. Sequence level distillation in particular\n",
      "would require a large compute budget. This makes them less practical for large scale applications.\n",
      "Whilst large language models produce better samples, intuitively there are \"easier\" tokens to predict\n",
      "for which smaller models may be sufficient. Therefore we may simply use a smaller version of the\n",
      "target language model as the draft and obtain high acceptance rates. This would also be convenient\n",
      "from an engineering and workflow perspective, since robust tooling for such models should already\n",
      "exist to train the target model in the first place.\n",
      "Results\n",
      "We train a 4 billion parameter draft model optimised for sampling latency on 16 TPU v4s ‚Äì the same\n",
      "hardware that is typically used to serve Chinchilla for research purposes. This model was trained\n",
      "with the same tokeniser and dataset as Chinchilla, with a slightly smaller width and with only 8\n",
      "layers. The relatively few number of layers allows it to achieve a sampling speed of 1.8ms/token\n",
      "comparedto14.1ms/tokenforChinchilla. Fordetails,pleaserefertothehyperparametersinTable2.\n",
      "For distributed setups it is insufficient to naively choose a small model as the draft, since differ-\n",
      "ent models have different optimal inference setups. For example, it is typical to serve Chinchilla 70B\n",
      "5AcceleratingLargeLanguageModelDecodingwithSpeculativeSampling\n",
      "Table1|ChinchillaperformanceandspeedonXSumandHumanEvalwithnaiveandspeculative\n",
      "sampling at batch size 1 and ùêæ = 4. XSum was executed with nucleus parameter ùëù = 0.8, and\n",
      "HumanEval with ùëù = 0.95 and temperature 0.8.\n",
      "Sampling Method Benchmark Result Mean Token Time Speed Up\n",
      "ArS (Nucleus) 0.112 14.1ms/Token 1√ó\n",
      "XSum (ROUGE-2)\n",
      "SpS (Nucleus) 0.114 7.52ms/Token 1.92√ó\n",
      "ArS (Greedy) 0.157 14.1ms/Token 1√ó\n",
      "XSum (ROUGE-2)\n",
      "SpS (Greedy) 0.156 7.00ms/Token 2.01√ó\n",
      "ArS (Nucleus) 45.1% 14.1ms/Token 1√ó\n",
      "HumanEval (100 Shot)\n",
      "SpS (Nucleus) 47.0% 5.73ms/Token 2.46√ó\n",
      "on16TPUv4s(whereitachievestheaforementioned14.1ms/token),whereasachinchilla-optimal\n",
      "7B achieves its lowest sampling latency on 4 TPU v4s (where it achieves 5ms/token). For smaller\n",
      "models,theadditionalmemorybandwidthandflopsareinsufficienttooffsettheadditionalcommuni-\n",
      "cation overhead between more devices ‚Äì serving a 7B on 16 TPUs actually increases the latency. This\n",
      "means the 7B would provide only a modest speedup if used as a draft with its optimal topology, and\n",
      "we will not make full utilisation of the hardware during drafting.\n",
      "We can sidestep this issue by training a wider model with a relatively few number of layers in\n",
      "order to minimise communication overhead. It has been observed that the performance of language\n",
      "models is relatively robust to changes in model aspect ratio (Levine et al., 2020), so this allows us\n",
      "to serve a powerful draft model which can be sampled rapidly on the same hardware as the target\n",
      "model.\n",
      "Evaluation on XSum and HumanEval\n",
      "We evaluate speculative sampling with Chinchilla on two tasks and summarize the results in Table 1:\n",
      "‚Ä¢ The XSum (Narayan et al., 2018) benchmark. This is a natural language summarisation task\n",
      "usinga1-shotpromptwhere wesamplea totalof 11,305sequenceswith amaximum sequence\n",
      "length 128.\n",
      "‚Ä¢ The 100-shot HumanEval task (Chen et al., 2021). This is a code generation task involves the\n",
      "generation of 16,400 samples with a maximum sequence length of 512.\n",
      "Even with greedy sampling, a single token deviating due to numerics could result in two sequences\n",
      "diverging wildly. Since pseudo-random seeds are processed differently between ArS and SpS, and\n",
      "because the different computation graphs lead to different numerics, we cannot not expect identical\n",
      "outputs. However, we expect the samples to come from the same distribution within numerics and\n",
      "we empirically verify this by evaluating these benchmarks.\n",
      "We run the tasks at batch size 1 with SpS and ArS. The time taken per SpS/ArS loop has low\n",
      "variance, and we can measure it directly from TPU profiles. To obtain the average speedup, standard\n",
      "deviations and other metrics, we log the amount of tokens generated for each speculative loop. In\n",
      "Table1weshowtheperformanceontheXSumandHumanEvalbenchmarksfornaiveandspeculative\n",
      "sampling with Chinchilla.\n",
      "6AcceleratingLargeLanguageModelDecodingwithSpeculativeSampling\n",
      "We obtain a substantial speedup in both tasks, with HumanEval reaching speedups of almost 2.5√ó.\n",
      "Yet, we have parity in the benchmark metrics ‚Äì the underlying samples distribution is provably the\n",
      "sameuptonumerics,andthisverifiesthatthedraftmodelisnotbiasingtheresultsempirically. Inthe\n",
      "case of HumanEval and greedy XSum, this speedup exceeded the theoretical memory bandwidth limit\n",
      "of the hardware for autoregressive sampling (model size divided by the total memory bandwidth).\n",
      "Acceptance rate changes per domain\n",
      "It is apparent that the acceptance rate is dependent on the application and the decoding method.\n",
      "HumanEvalachievesasignificantlylargerspeedup‚ÄîWehypothesizethatthisisduetoacombination\n",
      "of code containing a lot of common sub-sequences (e.g. for i in range(len(arr)): would be\n",
      "relatively easy for a draft model to guess), is often decomposed into a smaller set of shorter tokens\n",
      "and the temperature value sharpening both the draft and target logits.\n",
      "Mean Sampling Time (128 tokens) Acceptance rate Total loop time\n",
      "1800 Human Eval 1.0 Human Eval\n",
      "28\n",
      "XSum XSum\n",
      "1600 0.9 26\n",
      "1400 24\n",
      "0.8\n",
      "1200 22 sm sm\n",
      "0.7\n",
      "1000 20\n",
      "800 0.6 18\n",
      "16\n",
      "600 0.5\n",
      "14\n",
      "400\n",
      "0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7\n",
      "Number of draft tokens (K) Number of draft tokens (K) Number of draft tokens (K)\n",
      "Figure 1 | Left: The average time to generate 128 tokens, with standard deviation. Note that as ùêæ\n",
      "increases, the overall speedup plateaus or even regresses, with XSum being optimal at ùêæ = 3. The\n",
      "variance consistently increases with ùêæ. Middle: The average number of tokens accepted divided by\n",
      "ùêæ +1 ‚Äì this serves as a measure of the overall efficiency of the modified rejection scheme, which\n",
      "decreases with the lookahead. Right: Average time per loop increases approximately linearly with ùêæ\n",
      "duetotheincreasednumberofmodelcalls. Notethatthegradientisslightlyhigherthanthesampling\n",
      "speed of the draft model, due to additional overheads in nucleus decoding.\n",
      "Trade off between longer drafts and more frequent scoring\n",
      "Wevisualisethetrade-offofincreasingùêæ,thenumberoftokenssampledbythedraftmodelinFigure1.\n",
      "As ùêæ increases, we need fewer scoring calls from the large models to generate the same sequence\n",
      "length, potentially giving us a larger speedup. However, the total loop time increases approximately\n",
      "linearly with the larger number of draft model calls and small increases in the scoring time. The\n",
      "overall efficiency of the proportion of accepted tokens decreases as ùêæ increases, since later tokens\n",
      "depend on the acceptance of previous tokens. This results in the average speedup plateauing or\n",
      "even degrading with a larger ùêæ (for example, XSum with nucleus‚Äôs latency is minimised at ùêæ = 3),\n",
      "depending on the domain.\n",
      "7AcceleratingLargeLanguageModelDecodingwithSpeculativeSampling\n",
      "Further, even though larger values of ùêæ may yield marginally greater mean speedups in certain\n",
      "circumstances, it also increases variance of the time to generate a full sequence. This could be\n",
      "problematic for settings where the P90, P99 latencies of concern.\n",
      "Conclusion\n",
      "Inthiswork,wedemonstrateanewalgorithmandworkflowforacceleratingthedecodingoflanguage\n",
      "models. Speculative sampling does not require making any modifications to the target language\n",
      "model‚Äôs parameters or architecture, is provably lossless within numerics, scales well with the appro-\n",
      "priatedraftmodelandcomplementsmanyexistingtechniquesforreducinglatencyinthesmallbatch\n",
      "size setting.\n",
      "We optimise and scale the technique to Chinchilla 70B using a draft model which was easy to\n",
      "trainwithexistinginfrastructure,demonstratingthatityieldsalargespeedupacrossbenchmarktasks\n",
      "and common decoding methods in the process. We verify that it is indeed lossless empirically in its\n",
      "downstream tasks.\n",
      "References\n",
      "A.Arnab,M.Dehghani,G.Heigold,C.Sun,M.Lucic,andC.Schmid. Vivit: Avideovisiontransformer.\n",
      "In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 6816‚Äì6826. IEEE\n",
      "Computer Society, 2021.\n",
      "T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\n",
      "G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information\n",
      "processing systems, 33:1877‚Äì1901, 2020.\n",
      "M.Chen,J.Tworek,H.Jun,Q.Yuan,H.P.deOliveiraPinto,J.Kaplan,H.Edwards,Y.Burda,N.Joseph,\n",
      "G.Brockman,A.Ray,R.Puri,G.Krueger,M.Petrov,H.Khlaaf,G.Sastry,P.Mishkin,B.Chan,S.Gray,\n",
      "N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings,\n",
      "M.Plappert,F.Chantzis,E.Barnes,A.Herbert-Voss,W.H.Guss,A.Nichol,A.Paino,N.Tezak,J.Tang,\n",
      "I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra,\n",
      "E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew,\n",
      "D.Amodei,S.McCandlish,I.Sutskever,andW.Zaremba. Evaluatinglargelanguagemodelstrained\n",
      "on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.\n",
      "A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung,\n",
      "C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint\n",
      "arXiv:2204.02311, 2022.\n",
      "T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for\n",
      "transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n",
      "A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-\n",
      "derer,G.Heigold,S.Gelly,etal.Animageisworth16x16words: Transformersforimagerecognition\n",
      "at scale. arXiv preprint arXiv:2010.11929, 2020.\n",
      "T.Ge,H.Xia,X.Sun,S.Chen,andF.Wei. Losslessaccelerationforseq2seqgenerationwithaggressive\n",
      "decoding. ArXiv, abs/2205.10350, 2022.\n",
      "8AcceleratingLargeLanguageModelDecodingwithSpeculativeSampling\n",
      "J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.\n",
      "Hendricks,J.Welbl,A.Clark,etal. Trainingcompute-optimallargelanguagemodels. arXivpreprint\n",
      "arXiv:2203.15556, 2022.\n",
      "X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu. TinyBERT: Distilling BERT\n",
      "for natural language understanding. In Findings of the Association for Computational Linguis-\n",
      "tics: EMNLP 2020, pages 4163‚Äì4174, Online, Nov. 2020. Association for Computational Linguis-\n",
      "tics. doi: 10.18653/v1/2020.findings-emnlp.372. URL https://aclanthology.org/2020.\n",
      "findings-emnlp.372.\n",
      "Y. Kim and A. M. Rush. Sequence-level knowledge distillation. CoRR, abs/1606.07947, 2016. URL\n",
      "http://arxiv.org/abs/1606.07947.\n",
      "Y. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative decoding.\n",
      "ArXiv, abs/2211.17192, 2022.\n",
      "Y. Levine, N. Wies, O. Sharir, H. Bata, and A. Shashua. The depth-to-width interplay in self-attention.\n",
      "arXiv preprint arXiv:2006.12467, 2020.\n",
      "S. Narayan, S. B. Cohen, and M. Lapata. Don‚Äôt give me the details, just the summary! topic-\n",
      "aware convolutional neural networks for extreme summarization. In Proceedings of the 2018\n",
      "Conference on Empirical Methods in Natural Language Processing, pages 1797‚Äì1807, Brussels,\n",
      "Belgium, Oct.-Nov. 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1206.\n",
      "URL https://aclanthology.org/D18-1206.\n",
      "R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao, S. Agrawal,\n",
      "and J. Dean. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102, 2022.\n",
      "J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring,\n",
      "S. Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv\n",
      "preprint arXiv:2112.11446, 2021.\n",
      "V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster,\n",
      "cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n",
      "N.Shazeer. Fasttransformerdecoding: Onewrite-headisallyouneed. CoRR,abs/1911.02150,2019.\n",
      "URL http://arxiv.org/abs/1911.02150.\n",
      "M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training\n",
      "multi-billionparameterlanguagemodelsusingmodelparallelism. arXivpreprintarXiv:1909.08053,\n",
      "2019.\n",
      "Y.Song,C.Meng,R.Liao,andS.Ermon. Acceleratingfeedforwardcomputationviaparallelnonlinear\n",
      "equationsolving. InM.MeilaandT.Zhang,editors,Proceedingsofthe38thInternationalConference\n",
      "on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 9791‚Äì9800.\n",
      "PMLR, 18‚Äì24 Jul 2021. URL https://proceedings.mlr.press/v139/song21a.html.\n",
      "M. Stern, N. Shazeer, and J. Uszkoreit. Blockwise parallel decoding for deep autoregressive models.\n",
      "CoRR, abs/1811.03115, 2018. URL http://arxiv.org/abs/1811.03115.\n",
      "A. Wiggers and E. Hoogeboom. Predictive sampling with forecasting autoregressive models. In H. D.\n",
      "III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning,\n",
      "volume 119 of Proceedings of Machine Learning Research, pages 10260‚Äì10269. PMLR, 13‚Äì18 Jul\n",
      "2020. URL https://proceedings.mlr.press/v119/wiggers20a.html.\n",
      "9AcceleratingLargeLanguageModelDecodingwithSpeculativeSampling\n",
      "Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He. Zeroquant: Efficient and affordable\n",
      "post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.\n",
      "Supplementary Materials\n",
      "Author Contributions\n",
      "‚Ä¢ Initial proposal: Charlie Chen, John Jumper and Geoffrey Irving\n",
      "‚Ä¢ Initial Implementation, Optimisation and Scaling: Charlie Chen\n",
      "‚Ä¢ Modified Rejection Sampling Scheme: John Jumper\n",
      "‚Ä¢ Engineering Improvements: Jean-Baptiste Lespiau and Charlie Chen\n",
      "‚Ä¢ Experiments: Charlie Chen, Sebastian Borgeaud and Laurent Sifre\n",
      "‚Ä¢ Draft of Manuscript: Charlie Chen and Sebastian Borgeaud\n",
      "‚Ä¢ Manuscript Feedback: Laurent Sifre, Geoffrey Irving and John Jumper\n",
      "Acknowledgements\n",
      "We‚Äôd like to thank Oriol Vinyals and Koray Kavukcuoglu for your kind advice and leadership. We‚Äôd\n",
      "also like to thank Evan Senter for your additional feedback on the manuscript and Amelia Glaese for\n",
      "yoursupportinnavigatingthepublishingprocess. Finally,we‚ÄôdliketothankBlakeHechtman,Berkin\n",
      "Ilbeyi for your valuable advice on XLA and Nikolai Grigoriev for our discussions on the various tricks\n",
      "that can be applied to the transformer architecture.\n",
      "Hyperparams\n",
      "Table 2 | Hyperparameters for the draft model\n",
      "Model ùëë Heads Layers Params\n",
      "model\n",
      "Target (Chinchilla) 8192 64 80 70B\n",
      "Draft 6144 48 8 4B\n",
      "Proofs\n",
      "Theorem1(ModifiedRejectionSamplingrecoversthetargetdistribution). Givendiscretedistributions\n",
      "ùëû, ùëù and a single draft sample ùë•Àú‚àº ùëù, let ùëã be the final resulting sample. For ùëã = ùë• to be true, we must\n",
      "either sample ùë•Àú= ùë• and then accept it, or resample it after ùë•Àú(of any value) is rejected. Hence:\n",
      "‚Ñô(ùëã = ùë•)\n",
      "= ‚Ñô(ùë•Àú= ùë•)‚Ñô(ùë•Àúaccepted|ùë•Àú= ùë•)+‚Ñô(ùë•Àúrejected)‚Ñô(ùëã = ùë•|ùë•Àúrejected)\n",
      "For the first term, we apply the acceptance rule:\n",
      "‚Ñô(ùë•Àú= ùë•)‚Ñô(ùë•Àúaccepted|ùë•Àú= ùë•)\n",
      "(cid:18) ùëû(ùë•)(cid:19)\n",
      "= ùëù(ùë•)min 1,\n",
      "ùëù(ùë•)\n",
      "10AcceleratingLargeLanguageModelDecodingwithSpeculativeSampling\n",
      "= min(ùëù(ùë•),ùëû(ùë•))\n",
      "For the second conditional term, we apply the resampling rule:\n",
      "‚Ñô(ùëã = ùë•|ùë•Àúrejected) = (ùëû(ùë•)‚àí ùëù(ùë•))\n",
      "+\n",
      "Where (.) denotes:\n",
      "+\n",
      "max(0, ùëì(ùë•))\n",
      "(ùëì(ùë•)) =\n",
      "+ (cid:205) max(0, ùëì(ùë•))\n",
      "ùë•\n",
      "Finally, we calculate the probability of rejection:\n",
      "‚Ñô(ùë•Àúrejected) = 1‚àí‚Ñô(ùë•Àúaccepted)\n",
      "‚àëÔ∏Å\n",
      "= 1‚àí ‚Ñô(ùëã = ùë•(cid:48),ùë•Àúaccepted)\n",
      "ùë•(cid:48)\n",
      "‚àëÔ∏Å\n",
      "= 1‚àí min(ùëù(ùë•(cid:48)),ùëû(ùë•(cid:48)))\n",
      "ùë•(cid:48)\n",
      "‚àëÔ∏Å\n",
      "= max(0,ùëû(ùë•(cid:48))‚àí ùëù(ùë•(cid:48)))\n",
      "ùë•(cid:48)\n",
      "‚àëÔ∏Å\n",
      "= ùëû(ùë•(cid:48))‚àímin(ùëù(ùë•(cid:48)),ùëû(ùë•(cid:48)))\n",
      "ùë•(cid:48)\n",
      "‚àëÔ∏Å\n",
      "= max(0,ùëû(ùë•(cid:48))‚àí ùëù(ùë•(cid:48)))\n",
      "ùë•(cid:48)\n",
      "This is equal to the denominator of (ùëû(ùë•)‚àí ùëù(ùë•)) , so:\n",
      "+\n",
      "‚Ñô(ùë•Àúrejected)‚Ñô(ùëã = ùë•|ùë•Àúrejected) = max(0,ùëû(ùë•)‚àí ùëù(ùë•))\n",
      "Hence:\n",
      "‚Ñô(ùëã = ùë•)\n",
      "= min(ùëù(ùë•),ùëû(ùë•))+max(0,ùëû(ùë•)‚àí ùëù(ùë•))\n",
      "= ùëû(ùë•)\n",
      "and we have recovered the desired target.\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "with pdfplumber.open(pdf_file) as pdf:\n",
    "    extracted_text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        extracted_text += page.extract_text()\n",
    "\n",
    "# Print or use the extracted text as needed\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not _io.BytesIO",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocument_loaders\u001b[39;00m \u001b[39mimport\u001b[39;00m PDFPlumberLoader\n\u001b[0;32m----> 2\u001b[0m loader \u001b[39m=\u001b[39m PDFPlumberLoader(pdf_file)\n\u001b[1;32m      3\u001b[0m data \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39mload()\n",
      "File \u001b[0;32m~/personal/github/pyenv/arxiv-explainer/lib/python3.8/site-packages/langchain/document_loaders/pdf.py:461\u001b[0m, in \u001b[0;36mPDFPlumberLoader.__init__\u001b[0;34m(self, file_path, text_kwargs, dedupe, headers)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m    457\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpdfplumber package not found, please install it with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    458\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`pip install pdfplumber`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m     )\n\u001b[0;32m--> 461\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(file_path, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    462\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_kwargs \u001b[39m=\u001b[39m text_kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    463\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdedupe \u001b[39m=\u001b[39m dedupe\n",
      "File \u001b[0;32m~/personal/github/pyenv/arxiv-explainer/lib/python3.8/site-packages/langchain/document_loaders/pdf.py:83\u001b[0m, in \u001b[0;36mBasePDFLoader.__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexpanduser(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_path)\n\u001b[1;32m     82\u001b[0m \u001b[39m# If the file is a web path or S3, download it to a temporary file, and use that\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49misfile(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_path) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_valid_url(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_path):\n\u001b[1;32m     84\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemp_dir \u001b[39m=\u001b[39m tempfile\u001b[39m.\u001b[39mTemporaryDirectory()\n\u001b[1;32m     85\u001b[0m     _, suffix \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplitext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_path)\n",
      "File \u001b[0;32m/usr/lib/python3.8/genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Test whether a path is a regular file\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(path)\n\u001b[1;32m     31\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not _io.BytesIO"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "loader = PDFPlumberLoader(pdf_file)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "no such file: 'https://arxiv.org/pdf/2302.01318'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1077256/3088646909.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfitz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0marxiv_pdf_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://arxiv.org/pdf/2302.01318\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/personal/github/pyenv/arxiv-explainer/lib/python3.8/site-packages/fitz/fitz.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[0m\n\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfrom_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4112\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"no such file: '{filename}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4113\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4114\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4115\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"'{filename}' is no file\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4116\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mFileDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: no such file: 'https://arxiv.org/pdf/2302.01318'"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "\n",
    "arxiv_pdf_link = \"https://arxiv.org/pdf/2302.01318\"\n",
    "\n",
    "pdf_document = fitz.open(arxiv_pdf_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpk7qxhw20.pdf\n",
      "2023-2-3\n",
      "Accelerating Large Language Model Decoding\n",
      "with Speculative Sampling\n",
      "Charlie Chen1, Sebastian Borgeaud1, GeoÔ¨Ärey Irving1, Jean-Baptiste Lespiau1, Laurent Sifre1 and John\n",
      "Jumper1\n",
      "1All authors from DeepMind\n",
      "We present speculative sampling, an algorithm for accelerating transformer decoding by enabling the\n",
      "generation of multiple tokens from each transformer call. Our algorithm relies on the observation that\n",
      "the latency of parallel scoring of short continuations, generated by a faster but less powerful draft\n",
      "model, is comparable to that of sampling a single token from the larger target model. This is combined\n",
      "with a novel modiÔ¨Åed rejection sampling scheme which preserves the distribution of the target model\n",
      "within hardware numerics. We benchmark speculative sampling with Chinchilla, a 70 billion parameter\n",
      "language model, achieving a 2‚Äì2.5√ó decoding speedup in a distributed setup, without compromising\n",
      "the sample quality or making modiÔ¨Åcations to the model itself.\n",
      "Introduction\n",
      "Scaling transformer models to 500B+ parameters has led to large performance improvements on\n",
      "many natural language, computer vision and reinforcement learning tasks (Arnab et al., 2021; Brown\n",
      "et al., 2020; Chowdhery et al., 2022; Dosovitskiy et al., 2020; HoÔ¨Ämann et al., 2022; Rae et al., 2021).\n",
      "However, transformer decoding remains a highly costly and ineÔ¨Écient process in this regime.\n",
      "Transformer sampling is typically memory bandwidth bound (Shazeer, 2019), so for a given set of\n",
      "hardware, the time to generate a single token in transformer models is proportional to a Ô¨Årst order\n",
      "approximation to the size of parameters and the size of the transformer memory. The size of language\n",
      "models also necessitates serving with model parallelism ‚Äì adding communication overheads (Pope\n",
      "et al., 2022) and multiplying resource requirements. Since each new token depends on the past,\n",
      "many such transformer calls are required to sample a new sequence.\n",
      "We present an algorithm to accelerate transformer sampling for latency critical applications, which\n",
      "we call speculative sampling (SpS). This is achieved by:\n",
      "1. Generating a short draft of length ùê¥ÔøΩÔøΩÔøΩ. This can be attained with either a parallel model (Stern\n",
      "et al., 2018) or by calling a faster, auto-regressive model ùê¥ÔøΩÔøΩÔøΩ times. We shall refer to this model\n",
      "as the draft model, and focus on the case where it is auto-regressive.\n",
      "2. Scoring the draft using the larger, more powerful model from we wish to sample from. We shall\n",
      "refer to this model as the target model.\n",
      "3. Using a modiÔ¨Åed rejection sampling scheme, accept a subset of the ùê¥ÔøΩÔøΩÔøΩ draft tokens from left to\n",
      "right, recovering the distribution of the target model in the process.\n",
      "Intuitively, there are often sequences where the next token might be ‚Äúobvious‚Äù. Therefore, if there is\n",
      "strong agreement between the draft and target model‚Äôs distributions on a given token or sub-sequence\n",
      "of tokens, this setup permits the generation of multiple tokens each time the target model is called.\n",
      "We show that the expected acceptance rate of draft tokens is suÔ¨Écient to oÔ¨Äset the overhead of the\n",
      "Corresponding author(s): ccharlie@deepmind.com\n",
      "¬© 2023 DeepMind. All rights reserved\n",
      "arXiv:2302.01318v1  [cs.CL]  2 Feb 2023\n",
      "Accelerating Large Language Model Decoding with Speculative Sampling\n",
      "drafting process for large language models, resulting in an eÔ¨Äective and practical method for reducing\n",
      "sampling latency without the need for modifying the target model or biasing the sample distribution.\n",
      "Depending on the evaluation domain, SpS leads to a 2‚Äì2.5√ó speedup when sampling from Chinchilla\n",
      "(HoÔ¨Ämann et al., 2022). Notably, the mean tokens per second with SpS often exceeds the idealised\n",
      "ceiling on auto-regressive sampling speed imposed by the memory bandwidth.\n",
      "Related Work\n",
      "There has been a substantial body of work focused on improving sampling latency of large transform-\n",
      "ers and other auto-regressive models.\n",
      "Since sampling performance is heavily coupled with the model size in memory, quantisation to\n",
      "int8 or even int4 (Dettmers et al., 2022; Yao et al., 2022) and distillation (Jiao et al., 2020; Sanh\n",
      "et al., 2019) of transformers are eÔ¨Äective techniques for reducing sampling latency with little to no\n",
      "performance penalty. The observation that model size contributes less to the Ô¨Ånal performance than\n",
      "expected (HoÔ¨Ämann et al., 2022) has also encouraged smaller language models in general.\n",
      "During sampling, a cache of the keys and values is maintained for every attention layer, and could\n",
      "become a memory bandwidth bottleneck as the batch size increases. Methods such as multi-query\n",
      "attention (Shazeer, 2019) aims to improve sampling performance by shrinking this cache. However\n",
      "these techniques are most eÔ¨Äective at maximising throughout (at larger batch sizes) instead of latency,\n",
      "especially for larger models where the majority of the memory bandwidth budget is consumed by the\n",
      "parameters.\n",
      "Using a combination of the above techniques, in addition to a number of low-level optimisations to\n",
      "TPUs, Pope et al. (2022) have greatly improved the serving latency and eÔ¨Éciency of PaLM 540B.\n",
      "There is an existing body of similar work exploiting the eÔ¨Éciency of transformers and sequence\n",
      "models operating in parallel. This includes block parallel sampling (Stern et al., 2018), aggressive\n",
      "decoding (Ge et al., 2022), in addition to some work in parallelizing autoregressive models in the\n",
      "image domain (Song et al., 2021; Wiggers and Hoogeboom, 2020). These methods have yet to be\n",
      "adapted to typical language model use-cases since they either only work with greedy sampling, bias\n",
      "the results or are focused on other modalities. Further, to our knowledge none of these techniques\n",
      "have been scaled to distributed setups, which is necessary for the most expensive decoders with the\n",
      "tens or hundreds of billions of parameters.\n",
      "Coincidentally, the work in this manuscript was undertaken concurrently and independently of\n",
      "the work on speculative decoding from Leviathan et al. (2022). We focus more heavily the distributed\n",
      "serving setting for large models and oÔ¨Äer some incremental optimisations, but otherwise the core\n",
      "underlying idea is the same.\n",
      "Auto-regressive Sampling\n",
      "Whilst transformers can be trained eÔ¨Éciently and in parallel on TPUs and GPUs, samples are typically\n",
      "drawn auto-regressively (See algorithm 1). For most applications, auto-regressive sampling (ArS) is\n",
      "highly memory bandwidth bound and thus cannot make eÔ¨Äective use of modern accelerator hardware\n",
      "(Shazeer, 2019). A memory bound model call only generates a single token for every sequence in the\n",
      "batch, hence generating multiple tokens introduces a large amount of latency in any system which\n",
      "makes use of it.\n",
      "2\n",
      "Accelerating Large Language Model Decoding with Speculative Sampling\n",
      "This is especially problematic as the number of parameters in the model increases. Since all the model\n",
      "parameters need to pass through at least one accelerator chip, the model size divided by the total\n",
      "memory bandwidth across all chips gives us a hard ceiling on the maximum auto-regressive sampling\n",
      "speed. Larger models also require serving on multiple accelerators, introducing a further source of\n",
      "latency due to inter-device communication overheads.\n",
      "Algorithm 1 Auto-regressive (ArS) with Auto-Regressive Models\n",
      "Given auto-regressive target model ùëñÔøΩÔøΩÔøΩ(.|.) and initial prompt sequence ùëñÔøΩÔøΩÔøΩ1, . . . , ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ and target sequence\n",
      "length ùê¥ÔøΩÔøΩÔøΩ.\n",
      "Initialise ùëñÔøΩÔøΩÔøΩ ‚Üê ùëñÔøΩÔøΩÔøΩ.\n",
      "while ùëñÔøΩÔøΩÔøΩ < ùê¥ÔøΩÔøΩÔøΩ do\n",
      "Sample ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+1 ‚àº ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ|ùëñÔøΩÔøΩÔøΩ1, . . . , ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ)\n",
      "ùëñÔøΩÔøΩÔøΩ ‚Üê ùëñÔøΩÔøΩÔøΩ + 1\n",
      "end while\n",
      "Algorithm 2 Speculative Sampling (SpS) with Auto-Regressive Target and Draft Models\n",
      "Given lookahead ùê¥ÔøΩÔøΩÔøΩ and minimum target sequence length ùê¥ÔøΩÔøΩÔøΩ.\n",
      "Given auto-regressive target model ùëñÔøΩÔøΩÔøΩ(.|.), and auto-regressive draft model ùëñÔøΩÔøΩÔøΩ(.|.), initial prompt\n",
      "sequence ùëñÔøΩÔøΩÔøΩ0, . . . , ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ.\n",
      "Initialise ùëñÔøΩÔøΩÔøΩ ‚Üê ùëñÔøΩÔøΩÔøΩ.\n",
      "while ùëñÔøΩÔøΩÔøΩ < ùê¥ÔøΩÔøΩÔøΩ do\n",
      "for ùëñÔøΩÔøΩÔøΩ = 1 : ùê¥ÔøΩÔøΩÔøΩ do\n",
      "Sample draft auto-regressively ÀúùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ ‚àº ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ|, ùëñÔøΩÔøΩÔøΩ1, . . . , ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ, ÀúùëñÔøΩÔøΩÔøΩ1, . . . , ÀúùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ‚àí1)\n",
      "end for\n",
      "In parallel, compute ùê¥ÔøΩÔøΩÔøΩ + 1 sets of logits from drafts ÀúùëñÔøΩÔøΩÔøΩ1, . . . , ÀúùëñÔøΩÔøΩÔøΩùê¥ÔøΩÔøΩÔøΩ :\n",
      "ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ|, ùëñÔøΩÔøΩÔøΩ1, . . . , ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ), ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ|, ùëñÔøΩÔøΩÔøΩ1, . . . , ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ, ÀúùëñÔøΩÔøΩÔøΩ1), . . . , ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ|, ùëñÔøΩÔøΩÔøΩ1, . . . , ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ, ÀúùëñÔøΩÔøΩÔøΩ1, . . . , ÀúùëñÔøΩÔøΩÔøΩùê¥ÔøΩÔøΩÔøΩ)\n",
      "for ùëñÔøΩÔøΩÔøΩ = 1 : ùê¥ÔøΩÔøΩÔøΩ do\n",
      "Sample ùëñÔøΩÔøΩÔøΩ ‚àº ùê¥ÔøΩÔøΩÔøΩ[0, 1] from a uniform distribution.\n",
      "if ùëñÔøΩÔøΩÔøΩ < min\n",
      "ÔøΩ\n",
      "1, ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ |ùëñÔøΩÔøΩÔøΩ1,...,ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+ùëñÔøΩÔøΩÔøΩ‚àí1)\n",
      "ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ |ùëñÔøΩÔøΩÔøΩ1,...,ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+ùëñÔøΩÔøΩÔøΩ‚àí1)\n",
      "ÔøΩ\n",
      ", then\n",
      "Set ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+ùëñÔøΩÔøΩÔøΩ ‚Üê ÀúùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ and ùëñÔøΩÔøΩÔøΩ ‚Üê ùëñÔøΩÔøΩÔøΩ + 1.\n",
      "else\n",
      "sample ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+ùëñÔøΩÔøΩÔøΩ ‚àº (ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ|ùëñÔøΩÔøΩÔøΩ1, . . . , ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+ùëñÔøΩÔøΩÔøΩ‚àí1) ‚àí ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ|ùëñÔøΩÔøΩÔøΩ1, . . . , ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+ùëñÔøΩÔøΩÔøΩ‚àí1))+ and exit for loop.\n",
      "end if\n",
      "end for\n",
      "If all tokens ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+1, . . . , ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+ùê¥ÔøΩÔøΩÔøΩ are accepted, sample extra token ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+ùê¥ÔøΩÔøΩÔøΩ+1 ‚àº ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ|, ùëñÔøΩÔøΩÔøΩ1, . . . , ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ, ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+ùê¥ÔøΩÔøΩÔøΩ) and\n",
      "set ùëñÔøΩÔøΩÔøΩ ‚Üê ùëñÔøΩÔøΩÔøΩ + 1.\n",
      "end while\n",
      "Speculative Sampling\n",
      "Conditional Scoring\n",
      "For speculative sampling (See algorithm 2), we Ô¨Årst make the observation that computing the logits\n",
      "of a short continuation of ùê¥ÔøΩÔøΩÔøΩ tokens in parallel has a very similar latency to that of sampling a single\n",
      "3\n",
      "Accelerating Large Language Model Decoding with Speculative Sampling\n",
      "token. We focus our attention on large transformers, sharded in the Megatron style (Shoeybi et al.,\n",
      "2019). For these models the majority of sampling time can be attributed to three components:\n",
      "1. Linear Layers: For small batch sizes, each linear layer only processes a small number of\n",
      "embeddings. This causes the dense matrix multiplies in the feed-forward layers, queries, keys,\n",
      "values computations and the Ô¨Ånal attention projection to become memory bound. For small ùê¥ÔøΩÔøΩÔøΩ,\n",
      "this will continue to be memory bound and therefore take a similar amount of time.\n",
      "2. The Attention Mechanism: The attention mechanism is also memory bound. During sampling,\n",
      "we maintain a cache of all the keys and values of the previous tokens in the sequence to avoid\n",
      "re-computation. These KV-caches are large, and accounts for the majority of the memory\n",
      "bandwidth utilisation for the attention mechanism. However, since the KV-cache size does not\n",
      "change as we increase ùê¥ÔøΩÔøΩÔøΩ, there is little to no delta in this component.\n",
      "3. All-reduces: As models grow in size, its parameters need to be divided across multiple accelera-\n",
      "tors, leading to communication overheads. With Megatron, this manifests itself as an all-reduce\n",
      "after every feed-forward and attention layer. Since only the activations for a small number of\n",
      "tokens are transmitted, this operation is typically latency bound instead of throughput bound\n",
      "for both sampling and scoring (for small ùê¥ÔøΩÔøΩÔøΩ). Again, this results in a similar amount of time\n",
      "spent in the two cases.\n",
      "Other sources of overhead may exist, depending on the exact transformer implementation. Therefore\n",
      "it is still possible that the choice of positioning encoding, decoding method (e.g. a sort might be\n",
      "required for nucleus sampling), hardware limitations etc. can introduce some deltas between scoring\n",
      "and sampling. However, if the conditions are met such that the above components dominate then\n",
      "scoring should not be signiÔ¨Åcantly slower for small ùê¥ÔøΩÔøΩÔøΩ.\n",
      "ModiÔ¨Åed Rejection Sampling\n",
      "We require a method to recover the distribution of the target model from samples from the draft\n",
      "model, and logits of said tokens from both models.\n",
      "To achieve this, we introduce the following rejection sampling scheme of the drafted tokens. Given a\n",
      "sequence of tokens ùëñÔøΩÔøΩÔøΩ1, . . . , ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ, and ùê¥ÔøΩÔøΩÔøΩ draft tokens ÀúùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+1, . . . , ÀúùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+ùê¥ÔøΩÔøΩÔøΩ generated from ùëñÔøΩÔøΩÔøΩ(.|.), we accept ÀúùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+1\n",
      "with probability:\n",
      "min\n",
      "ÔøΩ\n",
      "1, ùëñÔøΩÔøΩÔøΩ(ÀúùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+1|ùëñÔøΩÔøΩÔøΩ1, . . . , ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ)\n",
      "ùëñÔøΩÔøΩÔøΩ(ÀúùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+1|ùëñÔøΩÔøΩÔøΩ1, . . . , ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ)\n",
      "ÔøΩ\n",
      "Where ùëñÔøΩÔøΩÔøΩ(ÀúùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+1|ùëñÔøΩÔøΩÔøΩ1, . . . , ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ) and ùëñÔøΩÔøΩÔøΩ(ÀúùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+1|ùëñÔøΩÔøΩÔøΩ1, . . . , ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ) are the probability of ÀúùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+1 according to the target and\n",
      "draft models respectively, conditioned on the context so far.\n",
      "If the token is accepted, we set ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+1 ‚Üê ÀúùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+1 and repeat the process for ÀúùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+2 until either a token\n",
      "is rejected or all tokens have been accepted.\n",
      "If ÀúùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+1 is rejected, we resample ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+1 from the following distribution:\n",
      "ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ+1 ‚àº (ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ|ùëñÔøΩÔøΩÔøΩ1, . . . , ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ) ‚àí ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ|ùëñÔøΩÔøΩÔøΩ1, . . . , ùëñÔøΩÔøΩÔøΩùëñÔøΩÔøΩÔøΩ))+\n",
      "Where (.)+ denotes:\n",
      "( ùê¥ÔøΩÔøΩÔøΩ (ùëñÔøΩÔøΩÔøΩ))+ =\n",
      "max(0, ùê¥ÔøΩÔøΩÔøΩ (ùëñÔøΩÔøΩÔøΩ))\n",
      "ÔøΩ\n",
      "ùëñÔøΩÔøΩÔøΩ max(0, ùê¥ÔøΩÔøΩÔøΩ (ùëñÔøΩÔøΩÔøΩ))\n",
      "By applying this sequentially, we recover the distribution of the target model for the accepted tokens\n",
      "(see proof in Theorem 1) within hardware numerics. Note that:\n",
      "4\n",
      "Accelerating Large Language Model Decoding with Speculative Sampling\n",
      "‚Ä¢ At least one token will always be generated from a draft-accept loop ‚Äì if the Ô¨Årst token is\n",
      "rejected, a valid token is resampled.\n",
      "‚Ä¢ Since the Ô¨Ånal token of the draft gives us the logits for the next token, if every drafted token is\n",
      "accepted, we can sample from it normally. This gives us a maximum of ùê¥ÔøΩÔøΩÔøΩ + 1 tokens per loop,\n",
      "over the naive implementation which would only return ùê¥ÔøΩÔøΩÔøΩ tokens.\n",
      "With standard sampling methods such as nucleus, top-k sampling and adjusting temperature, we\n",
      "can modify the probabilities accordingly before applying this rejection sampling scheme. We have\n",
      "observed that the overall acceptance rate is robust to the exact parameters used.\n",
      "Because we do not interact with the body of the transformer itself, this method can be used in\n",
      "conjunction many other techniques for accelerating or optimising the memory use of sampling, such\n",
      "as quantisation and multi-query attention.\n",
      "Choice of Draft Models\n",
      "Since the acceptance criterion guarantees the distribution of the target model in our samples, we are\n",
      "free to choose the method for drafting a continuation as long as it exposes logits, and there is a high\n",
      "enough acceptance rate and/or low enough latency to break-even. There exist several approaches\n",
      "here:\n",
      "‚Ä¢ Incorporating draft generation into the target model, and train the model from the start. This\n",
      "is the strategy used by Stern et al. (2018), which adds multiple heads into the transformer to\n",
      "generate multiple tokens.\n",
      "‚Ä¢ Using sequence level distillation (Kim and Rush, 2016) to generate a second model which\n",
      "predicts ùê¥ÔøΩÔøΩÔøΩ tokens in parallel. This strategy was employed by Ge et al. (2022).\n",
      "‚Ä¢ Set a portion of the activations of the target model as an input to the draft model, and train the\n",
      "draft model with this input.\n",
      "Although these methods will likely yield powerful drafts, they require a large number of data gener-\n",
      "ated from the target model or changes to the target model. Sequence level distillation in particular\n",
      "would require a large compute budget. This makes them less practical for large scale applications.\n",
      "Whilst large language models produce better samples, intuitively there are \"easier\" tokens to predict\n",
      "for which smaller models may be suÔ¨Écient. Therefore we may simply use a smaller version of the\n",
      "target language model as the draft and obtain high acceptance rates. This would also be convenient\n",
      "from an engineering and workÔ¨Çow perspective, since robust tooling for such models should already\n",
      "exist to train the target model in the Ô¨Årst place.\n",
      "Results\n",
      "We train a 4 billion parameter draft model optimised for sampling latency on 16 TPU v4s ‚Äì the same\n",
      "hardware that is typically used to serve Chinchilla for research purposes. This model was trained\n",
      "with the same tokeniser and dataset as Chinchilla, with a slightly smaller width and with only 8\n",
      "layers. The relatively few number of layers allows it to achieve a sampling speed of 1.8ms/token\n",
      "compared to 14.1ms/token for Chinchilla. For details, please refer to the hyperparameters in Table 2.\n",
      "For distributed setups it is insuÔ¨Écient to naively choose a small model as the draft, since diÔ¨Äer-\n",
      "ent models have diÔ¨Äerent optimal inference setups. For example, it is typical to serve Chinchilla 70B\n",
      "5\n",
      "Accelerating Large Language Model Decoding with Speculative Sampling\n",
      "Table 1 | Chinchilla performance and speed on XSum and HumanEval with naive and speculative\n",
      "sampling at batch size 1 and ùê¥ÔøΩÔøΩÔøΩ = 4. XSum was executed with nucleus parameter ùëñÔøΩÔøΩÔøΩ = 0.8, and\n",
      "HumanEval with ùëñÔøΩÔøΩÔøΩ = 0.95 and temperature 0.8.\n",
      "Sampling Method\n",
      "Benchmark\n",
      "Result\n",
      "Mean Token Time\n",
      "Speed Up\n",
      "ArS (Nucleus)\n",
      "XSum (ROUGE-2)\n",
      "0.112\n",
      "14.1ms/Token\n",
      "1√ó\n",
      "SpS (Nucleus)\n",
      "0.114\n",
      "7.52ms/Token\n",
      "1.92√ó\n",
      "ArS (Greedy)\n",
      "XSum (ROUGE-2)\n",
      "0.157\n",
      "14.1ms/Token\n",
      "1√ó\n",
      "SpS (Greedy)\n",
      "0.156\n",
      "7.00ms/Token\n",
      "2.01√ó\n",
      "ArS (Nucleus)\n",
      "HumanEval (100 Shot)\n",
      "45.1%\n",
      "14.1ms/Token\n",
      "1√ó\n",
      "SpS (Nucleus)\n",
      "47.0%\n",
      "5.73ms/Token\n",
      "2.46√ó\n",
      "on 16 TPU v4s (where it achieves the aforementioned 14.1ms/token), whereas a chinchilla-optimal\n",
      "7B achieves its lowest sampling latency on 4 TPU v4s (where it achieves 5ms/token). For smaller\n",
      "models, the additional memory bandwidth and Ô¨Çops are insuÔ¨Écient to oÔ¨Äset the additional communi-\n",
      "cation overhead between more devices ‚Äì serving a 7B on 16 TPUs actually increases the latency. This\n",
      "means the 7B would provide only a modest speedup if used as a draft with its optimal topology, and\n",
      "we will not make full utilisation of the hardware during drafting.\n",
      "We can sidestep this issue by training a wider model with a relatively few number of layers in\n",
      "order to minimise communication overhead. It has been observed that the performance of language\n",
      "models is relatively robust to changes in model aspect ratio (Levine et al., 2020), so this allows us\n",
      "to serve a powerful draft model which can be sampled rapidly on the same hardware as the target\n",
      "model.\n",
      "Evaluation on XSum and HumanEval\n",
      "We evaluate speculative sampling with Chinchilla on two tasks and summarize the results in Table 1:\n",
      "‚Ä¢ The XSum (Narayan et al., 2018) benchmark. This is a natural language summarisation task\n",
      "using a 1-shot prompt where we sample a total of 11,305 sequences with a maximum sequence\n",
      "length 128.\n",
      "‚Ä¢ The 100-shot HumanEval task (Chen et al., 2021). This is a code generation task involves the\n",
      "generation of 16,400 samples with a maximum sequence length of 512.\n",
      "Even with greedy sampling, a single token deviating due to numerics could result in two sequences\n",
      "diverging wildly. Since pseudo-random seeds are processed diÔ¨Äerently between ArS and SpS, and\n",
      "because the diÔ¨Äerent computation graphs lead to diÔ¨Äerent numerics, we cannot not expect identical\n",
      "outputs. However, we expect the samples to come from the same distribution within numerics and\n",
      "we empirically verify this by evaluating these benchmarks.\n",
      "We run the tasks at batch size 1 with SpS and ArS. The time taken per SpS/ArS loop has low\n",
      "variance, and we can measure it directly from TPU proÔ¨Åles. To obtain the average speedup, standard\n",
      "deviations and other metrics, we log the amount of tokens generated for each speculative loop. In\n",
      "Table 1 we show the performance on the XSum and HumanEval benchmarks for naive and speculative\n",
      "sampling with Chinchilla.\n",
      "6\n",
      "Accelerating Large Language Model Decoding with Speculative Sampling\n",
      "We obtain a substantial speedup in both tasks, with HumanEval reaching speedups of almost 2.5√ó.\n",
      "Yet, we have parity in the benchmark metrics ‚Äì the underlying samples distribution is provably the\n",
      "same up to numerics, and this veriÔ¨Åes that the draft model is not biasing the results empirically. In the\n",
      "case of HumanEval and greedy XSum, this speedup exceeded the theoretical memory bandwidth limit\n",
      "of the hardware for autoregressive sampling (model size divided by the total memory bandwidth).\n",
      "Acceptance rate changes per domain\n",
      "It is apparent that the acceptance rate is dependent on the application and the decoding method.\n",
      "HumanEval achieves a signiÔ¨Åcantly larger speedup ‚Äî We hypothesize that this is due to a combination\n",
      "of code containing a lot of common sub-sequences (e.g. for i in range(len(arr)): would be\n",
      "relatively easy for a draft model to guess), is often decomposed into a smaller set of shorter tokens\n",
      "and the temperature value sharpening both the draft and target logits.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Number of draft tokens (K)\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "ms\n",
      "Mean Sampling Time (128 tokens)\n",
      "Human Eval\n",
      "XSum\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Number of draft tokens (K)\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "Acceptance rate\n",
      "Human Eval\n",
      "XSum\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Number of draft tokens (K)\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "ms\n",
      "Total loop time\n",
      "Figure 1 | Left: The average time to generate 128 tokens, with standard deviation. Note that as ùê¥ÔøΩÔøΩÔøΩ\n",
      "increases, the overall speedup plateaus or even regresses, with XSum being optimal at ùê¥ÔøΩÔøΩÔøΩ = 3. The\n",
      "variance consistently increases with ùê¥ÔøΩÔøΩÔøΩ. Middle: The average number of tokens accepted divided by\n",
      "ùê¥ÔøΩÔøΩÔøΩ + 1 ‚Äì this serves as a measure of the overall eÔ¨Éciency of the modiÔ¨Åed rejection scheme, which\n",
      "decreases with the lookahead. Right: Average time per loop increases approximately linearly with ùê¥ÔøΩÔøΩÔøΩ\n",
      "due to the increased number of model calls. Note that the gradient is slightly higher than the sampling\n",
      "speed of the draft model, due to additional overheads in nucleus decoding.\n",
      "Trade oÔ¨Ä between longer drafts and more frequent scoring\n",
      "We visualise the trade-oÔ¨Ä of increasing ùê¥ÔøΩÔøΩÔøΩ, the number of tokens sampled by the draft model in Figure 1.\n",
      "As ùê¥ÔøΩÔøΩÔøΩ increases, we need fewer scoring calls from the large models to generate the same sequence\n",
      "length, potentially giving us a larger speedup. However, the total loop time increases approximately\n",
      "linearly with the larger number of draft model calls and small increases in the scoring time. The\n",
      "overall eÔ¨Éciency of the proportion of accepted tokens decreases as ùê¥ÔøΩÔøΩÔøΩ increases, since later tokens\n",
      "depend on the acceptance of previous tokens. This results in the average speedup plateauing or\n",
      "even degrading with a larger ùê¥ÔøΩÔøΩÔøΩ (for example, XSum with nucleus‚Äôs latency is minimised at ùê¥ÔøΩÔøΩÔøΩ = 3),\n",
      "depending on the domain.\n",
      "7\n",
      "Accelerating Large Language Model Decoding with Speculative Sampling\n",
      "Further, even though larger values of ùê¥ÔøΩÔøΩÔøΩ may yield marginally greater mean speedups in certain\n",
      "circumstances, it also increases variance of the time to generate a full sequence. This could be\n",
      "problematic for settings where the P90, P99 latencies of concern.\n",
      "Conclusion\n",
      "In this work, we demonstrate a new algorithm and workÔ¨Çow for accelerating the decoding of language\n",
      "models. Speculative sampling does not require making any modiÔ¨Åcations to the target language\n",
      "model‚Äôs parameters or architecture, is provably lossless within numerics, scales well with the appro-\n",
      "priate draft model and complements many existing techniques for reducing latency in the small batch\n",
      "size setting.\n",
      "We optimise and scale the technique to Chinchilla 70B using a draft model which was easy to\n",
      "train with existing infrastructure, demonstrating that it yields a large speedup across benchmark tasks\n",
      "and common decoding methods in the process. We verify that it is indeed lossless empirically in its\n",
      "downstream tasks.\n",
      "References\n",
      "A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lucic, and C. Schmid. Vivit: A video vision transformer.\n",
      "In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 6816‚Äì6826. IEEE\n",
      "Computer Society, 2021.\n",
      "T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\n",
      "G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information\n",
      "processing systems, 33:1877‚Äì1901, 2020.\n",
      "M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph,\n",
      "G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray,\n",
      "N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings,\n",
      "M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang,\n",
      "I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra,\n",
      "E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew,\n",
      "D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained\n",
      "on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.\n",
      "A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung,\n",
      "C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint\n",
      "arXiv:2204.02311, 2022.\n",
      "T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for\n",
      "transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n",
      "A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-\n",
      "derer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition\n",
      "at scale. arXiv preprint arXiv:2010.11929, 2020.\n",
      "T. Ge, H. Xia, X. Sun, S. Chen, and F. Wei. Lossless acceleration for seq2seq generation with aggressive\n",
      "decoding. ArXiv, abs/2205.10350, 2022.\n",
      "8\n",
      "Accelerating Large Language Model Decoding with Speculative Sampling\n",
      "J. HoÔ¨Ämann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.\n",
      "Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint\n",
      "arXiv:2203.15556, 2022.\n",
      "X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu. TinyBERT: Distilling BERT\n",
      "for natural language understanding. In Findings of the Association for Computational Linguis-\n",
      "tics: EMNLP 2020, pages 4163‚Äì4174, Online, Nov. 2020. Association for Computational Linguis-\n",
      "tics. doi: 10.18653/v1/2020.Ô¨Åndings-emnlp.372. URL https://aclanthology.org/2020.\n",
      "findings-emnlp.372.\n",
      "Y. Kim and A. M. Rush. Sequence-level knowledge distillation. CoRR, abs/1606.07947, 2016. URL\n",
      "http://arxiv.org/abs/1606.07947.\n",
      "Y. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative decoding.\n",
      "ArXiv, abs/2211.17192, 2022.\n",
      "Y. Levine, N. Wies, O. Sharir, H. Bata, and A. Shashua. The depth-to-width interplay in self-attention.\n",
      "arXiv preprint arXiv:2006.12467, 2020.\n",
      "S. Narayan, S. B. Cohen, and M. Lapata.\n",
      "Don‚Äôt give me the details, just the summary! topic-\n",
      "aware convolutional neural networks for extreme summarization. In Proceedings of the 2018\n",
      "Conference on Empirical Methods in Natural Language Processing, pages 1797‚Äì1807, Brussels,\n",
      "Belgium, Oct.-Nov. 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1206.\n",
      "URL https://aclanthology.org/D18-1206.\n",
      "R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao, S. Agrawal,\n",
      "and J. Dean. EÔ¨Éciently scaling transformer inference. arXiv preprint arXiv:2211.05102, 2022.\n",
      "J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. HoÔ¨Ämann, F. Song, J. Aslanides, S. Henderson, R. Ring,\n",
      "S. Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv\n",
      "preprint arXiv:2112.11446, 2021.\n",
      "V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster,\n",
      "cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n",
      "N. Shazeer. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150, 2019.\n",
      "URL http://arxiv.org/abs/1911.02150.\n",
      "M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training\n",
      "multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053,\n",
      "2019.\n",
      "Y. Song, C. Meng, R. Liao, and S. Ermon. Accelerating feedforward computation via parallel nonlinear\n",
      "equation solving. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference\n",
      "on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 9791‚Äì9800.\n",
      "PMLR, 18‚Äì24 Jul 2021. URL https://proceedings.mlr.press/v139/song21a.html.\n",
      "M. Stern, N. Shazeer, and J. Uszkoreit. Blockwise parallel decoding for deep autoregressive models.\n",
      "CoRR, abs/1811.03115, 2018. URL http://arxiv.org/abs/1811.03115.\n",
      "A. Wiggers and E. Hoogeboom. Predictive sampling with forecasting autoregressive models. In H. D.\n",
      "III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning,\n",
      "volume 119 of Proceedings of Machine Learning Research, pages 10260‚Äì10269. PMLR, 13‚Äì18 Jul\n",
      "2020. URL https://proceedings.mlr.press/v119/wiggers20a.html.\n",
      "9\n",
      "Accelerating Large Language Model Decoding with Speculative Sampling\n",
      "Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He. Zeroquant: EÔ¨Écient and aÔ¨Äordable\n",
      "post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.\n",
      "Supplementary Materials\n",
      "Author Contributions\n",
      "‚Ä¢ Initial proposal: Charlie Chen, John Jumper and GeoÔ¨Ärey Irving\n",
      "‚Ä¢ Initial Implementation, Optimisation and Scaling: Charlie Chen\n",
      "‚Ä¢ ModiÔ¨Åed Rejection Sampling Scheme: John Jumper\n",
      "‚Ä¢ Engineering Improvements: Jean-Baptiste Lespiau and Charlie Chen\n",
      "‚Ä¢ Experiments: Charlie Chen, Sebastian Borgeaud and Laurent Sifre\n",
      "‚Ä¢ Draft of Manuscript: Charlie Chen and Sebastian Borgeaud\n",
      "‚Ä¢ Manuscript Feedback: Laurent Sifre, GeoÔ¨Ärey Irving and John Jumper\n",
      "Acknowledgements\n",
      "We‚Äôd like to thank Oriol Vinyals and Koray Kavukcuoglu for your kind advice and leadership. We‚Äôd\n",
      "also like to thank Evan Senter for your additional feedback on the manuscript and Amelia Glaese for\n",
      "your support in navigating the publishing process. Finally, we‚Äôd like to thank Blake Hechtman, Berkin\n",
      "Ilbeyi for your valuable advice on XLA and Nikolai Grigoriev for our discussions on the various tricks\n",
      "that can be applied to the transformer architecture.\n",
      "Hyperparams\n",
      "Table 2 | Hyperparameters for the draft model\n",
      "Model\n",
      "ùê¥ÔøΩÔøΩÔøΩmodel\n",
      "Heads\n",
      "Layers\n",
      "Params\n",
      "Target (Chinchilla)\n",
      "8192\n",
      "64\n",
      "80\n",
      "70B\n",
      "Draft\n",
      "6144\n",
      "48\n",
      "8\n",
      "4B\n",
      "Proofs\n",
      "Theorem 1 (ModiÔ¨Åed Rejection Sampling recovers the target distribution). Given discrete distributions\n",
      "ùëñÔøΩÔøΩÔøΩ, ùëñÔøΩÔøΩÔøΩ and a single draft sample ÀúùëñÔøΩÔøΩÔøΩ ‚àº ùëñÔøΩÔøΩÔøΩ, let ùê¥ÔøΩÔøΩÔøΩ be the Ô¨Ånal resulting sample. For ùê¥ÔøΩÔøΩÔøΩ = ùëñÔøΩÔøΩÔøΩ to be true, we must\n",
      "either sample ÀúùëñÔøΩÔøΩÔøΩ = ùëñÔøΩÔøΩÔøΩ and then accept it, or resample it after ÀúùëñÔøΩÔøΩÔøΩ (of any value) is rejected. Hence:\n",
      "‚Ñô(ùê¥ÔøΩÔøΩÔøΩ = ùëñÔøΩÔøΩÔøΩ)\n",
      "= ‚Ñô(ÀúùëñÔøΩÔøΩÔøΩ = ùëñÔøΩÔøΩÔøΩ)‚Ñô(ÀúùëñÔøΩÔøΩÔøΩ accepted|ÀúùëñÔøΩÔøΩÔøΩ = ùëñÔøΩÔøΩÔøΩ) + ‚Ñô(ÀúùëñÔøΩÔøΩÔøΩ rejected)‚Ñô(ùê¥ÔøΩÔøΩÔøΩ = ùëñÔøΩÔøΩÔøΩ|ÀúùëñÔøΩÔøΩÔøΩ rejected)\n",
      "For the Ô¨Årst term, we apply the acceptance rule:\n",
      "‚Ñô(ÀúùëñÔøΩÔøΩÔøΩ = ùëñÔøΩÔøΩÔøΩ)‚Ñô(ÀúùëñÔøΩÔøΩÔøΩ accepted|ÀúùëñÔøΩÔøΩÔøΩ = ùëñÔøΩÔøΩÔøΩ)\n",
      "= ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ) min\n",
      "ÔøΩ\n",
      "1, ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ)\n",
      "ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ)\n",
      "ÔøΩ\n",
      "10\n",
      "Accelerating Large Language Model Decoding with Speculative Sampling\n",
      "= min (ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ), ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ))\n",
      "For the second conditional term, we apply the resampling rule:\n",
      "‚Ñô(ùê¥ÔøΩÔøΩÔøΩ = ùëñÔøΩÔøΩÔøΩ|ÀúùëñÔøΩÔøΩÔøΩ rejected) = (ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ) ‚àí ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ))+\n",
      "Where (.)+ denotes:\n",
      "( ùê¥ÔøΩÔøΩÔøΩ (ùëñÔøΩÔøΩÔøΩ))+ =\n",
      "max(0, ùê¥ÔøΩÔøΩÔøΩ (ùëñÔøΩÔøΩÔøΩ))\n",
      "ÔøΩ\n",
      "ùëñÔøΩÔøΩÔøΩ max(0, ùê¥ÔøΩÔøΩÔøΩ (ùëñÔøΩÔøΩÔøΩ))\n",
      "Finally, we calculate the probability of rejection:\n",
      "‚Ñô(ÀúùëñÔøΩÔøΩÔøΩ rejected) = 1 ‚àí ‚Ñô(ÀúùëñÔøΩÔøΩÔøΩ accepted)\n",
      "= 1 ‚àí\n",
      "‚àëÔ∏Å\n",
      "ùëñÔøΩÔøΩÔøΩ‚Ä≤\n",
      "‚Ñô(ùê¥ÔøΩÔøΩÔøΩ = ùëñÔøΩÔøΩÔøΩ‚Ä≤, ÀúùëñÔøΩÔøΩÔøΩ accepted)\n",
      "= 1 ‚àí\n",
      "‚àëÔ∏Å\n",
      "ùëñÔøΩÔøΩÔøΩ‚Ä≤\n",
      "min(ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ‚Ä≤), ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ‚Ä≤))\n",
      "=\n",
      "‚àëÔ∏Å\n",
      "ùëñÔøΩÔøΩÔøΩ‚Ä≤\n",
      "max(0, ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ‚Ä≤) ‚àí ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ‚Ä≤))\n",
      "=\n",
      "‚àëÔ∏Å\n",
      "ùëñÔøΩÔøΩÔøΩ‚Ä≤\n",
      "ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ‚Ä≤) ‚àí min(ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ‚Ä≤), ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ‚Ä≤))\n",
      "=\n",
      "‚àëÔ∏Å\n",
      "ùëñÔøΩÔøΩÔøΩ‚Ä≤\n",
      "max(0, ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ‚Ä≤) ‚àí ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ‚Ä≤))\n",
      "This is equal to the denominator of (ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ) ‚àí ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ))+, so:\n",
      "‚Ñô(ÀúùëñÔøΩÔøΩÔøΩ rejected)‚Ñô(ùê¥ÔøΩÔøΩÔøΩ = ùëñÔøΩÔøΩÔøΩ|ÀúùëñÔøΩÔøΩÔøΩ rejected) = max(0, ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ) ‚àí ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ))\n",
      "Hence:\n",
      "‚Ñô(ùê¥ÔøΩÔøΩÔøΩ = ùëñÔøΩÔøΩÔøΩ)\n",
      "= min(ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ), ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ)) + max(0, ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ) ‚àí ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ))\n",
      "= ùëñÔøΩÔøΩÔøΩ(ùëñÔøΩÔøΩÔøΩ)\n",
      "and we have recovered the desired target.\n",
      "11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(arxiv_pdf_link)\n",
    "if response.status_code == 200:\n",
    "    # Create a temporary file to store the PDF\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as temp_pdf_file:\n",
    "        temp_pdf_file.write(response.content)\n",
    "        pdf_path = temp_pdf_file.name\n",
    "        print(pdf_path)\n",
    "    \n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "    # Initialize an empty string to store the extracted text\n",
    "    extracted_text = \"\"\n",
    "\n",
    "    # Iterate through pages and extract text\n",
    "    for page_number in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_number)\n",
    "        extracted_text += page.get_text()\n",
    "    \n",
    "    pdf_document.close()\n",
    "    os.remove(pdf_path)\n",
    "\n",
    "    print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-explainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75f1c72865f84540d3caa5b2d750aba2f8b2927e817e25617cf0568c81dfc598"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
